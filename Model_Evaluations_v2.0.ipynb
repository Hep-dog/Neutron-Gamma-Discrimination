{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Model_Evaluations_v2.0.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jyrV8lh6-DDt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"IWsH74tgC6pZ","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1588796576705,"user_tz":240,"elapsed":26458,"user":{"displayName":"Daniel Ruiz","photoUrl":"","userId":"00785095761915236572"}},"outputId":"b87a3cf7-62c7-467c-9f41-37e0ddc61b42"},"source":["# general imports\n","import os\n","import sys\n","import io\n","import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","# ML imports\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import normalize, Normalizer\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, InputLayer\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n","from tensorflow.keras.optimizers import RMSprop\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","\n","# filepath for project folder\n","path = '/content/drive/My Drive/RESEARCH/NG_DISCRIMINATION/'\n","\n","# to play sound when cell finishes executing\n","from IPython.display import Audio\n","sound_file1 = path + 'wow.mp3'\n","sound_file2 = path + 'ding.mp3'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"prb7Ye510n8L"},"source":["### Contents"]},{"cell_type":"code","metadata":{"id":"JxcokzSc0AWq"},"source":["def evaluate_NG(model, train_labels, train_data, evaluation_labels, evaluation_data, metaModel, metaCollection):\n","  print(metaModel, metaCollection)\n","\n","  #print(\"Loading training data...\")\n","  td = np.load(train_data)\n","\n","  #print(\"Extracting training sequences...\")\n","  train_seqs = []\n","  for i in range(len(td)):\n","      train_seqs.append(td[i][11])\n","  train_seqs = np.array(train_seqs)\n","  train_seqs = train_seqs.astype('float64')\n","\n","  #print(\"Loading training label data...\")\n","  tl = np.load(train_labels)\n","\n","  #print(\"Extracting training labels...\")\n","  train_labels = []\n","  for i in range(len(tl)):\n","      if tl[i][10] == 1:\n","          train_labels.append(0)\n","      elif tl[i][10] == 2:\n","          train_labels.append(1)\n","  train_labels = np.array(train_labels)\n","\n","  #print(\"Calculating normalization parameters...\")\n","  xtrain, xtest, ytrain, ytest = train_test_split(train_seqs, train_labels, test_size = .2, shuffle = True, random_state = 8)\n","  normalizer = Normalizer(copy = False).fit(xtrain)\n","\n","  #print(\"Loading evaluation data...\")\n","  ed = np.load(evaluation_data)\n","\n","  #print(\"Extracting evaluation sequences...\")\n","  eva_seqs = []\n","  for i in range(len(ed)):\n","      eva_seqs.append(ed[i][11])\n","  eva_seqs = np.array(eva_seqs)\n","  eva_seqs = eva_seqs.astype('float64')\n","\n","  #print(\"Normalizing...\")\n","  n_eva_seqs = normalizer.transform(eva_seqs)\n","  n_eva_seqs = np.reshape(n_eva_seqs, (n_eva_seqs.shape[0], n_eva_seqs.shape[1], 1))\n","\n","  #print(\"Loading evaluation label data...\")\n","  el = np.load(evaluation_labels)\n","\n","  #print(\"Extracting evaluation labels...\")\n","  eva_labels = []\n","  for i in range(len(el)):\n","      if el[i][10] == 1:\n","          eva_labels.append(0)\n","      elif el[i][10] == 2:\n","          eva_labels.append(1)\n","  eva_labels = np.array(eva_labels)\n","\n","  #print(\"Loading model...\")\n","  trained_model = load_model(model)\n","\n","  #print(\"Conducting evaluation...\")\n","  results = trained_model.evaluate(x = n_eva_seqs, y = eva_labels, batch_size = 16, verbose = 1)\n","  \n","  print(\"Results:\")\n","  print('test loss, test acc:', results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkntaUkE3JkJ"},"source":["### Run Evaluation"]},{"cell_type":"code","metadata":{"id":"HuV6De8y29Yb"},"source":["pModel = path + \"models/WLS_v2.0.hdf5\"\n","pTrainingLabels = path + \"datasets/202002192219/ols.npy\" # do not change\n","pTrainingData = path + \"datasets/202002192219/wls.npy\" # do not change\n","pEvalLabels = path + \"datasets/202002192219/ols.npy\"\n","pEvalData = path + \"datasets/202002192219/wls.npy\"\n","#evaluate_NG(pModel, pTraining, pLabels, pEval)\n","\n","Models = ['WLS_v1.0.hdf5','WLS_v2.0.hdf5','WLS_v3.0.hdf5']\n","Collections = ['202002192219','202002200023','202002192353','202002192255','202002200054','202002192326']\n","\n","for i in range(len(Models)):\n","  Audio(sound_file1, autoplay=True)\n","  print('////////////////////////////////////////////////////////////////////////////////////////////')\n","  for j in range(len(Collections)):\n","    evaluate_NG(path + 'models/' + Models[i],                       # model\n","                pTrainingLabels,                                    # training labels (get thrown out)\n","                pTrainingData,                                      # training data (used to calculate normalization parameters)\n","                path + 'datasets/' + Collections[j] + '/ols.npy',   # evaluation data\n","                path + 'datasets/' + Collections[j] + '/wls.npy',   # evaluation labels\n","                Models[i],                                          # model name\n","                Collections[j])                                     # collection date\n","    Audio(sound_file2, autoplay=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meFgS5cXoQeV","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1588796698458,"user_tz":240,"elapsed":95712,"user":{"displayName":"Daniel Ruiz","photoUrl":"","userId":"00785095761915236572"}},"outputId":"badfd2fe-cbc2-4ef9-c4fb-aad2a3871b17"},"source":["evaluate_NG(path + \"models/WLS_1F_v1.0.hdf5\",\n","            path + \"datasets/202002192353/ols.npy\",\n","            path + \"datasets/202002192353/wls.npy\",\n","            path + \"datasets/202002192353/ols.npy\",\n","            path + \"datasets/202002192353/wls.npy\",\n","            \"WLS_1F_v1.0.hdf5\",\n","            \"202002192353\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WLS_1F_v1.0.hdf5 202002192353\n","11141/11141 [==============================] - 78s 7ms/step - loss: 0.3840 - accuracy: 0.8392\n","Results:\n","test loss, test acc: [0.3840163052082062, 0.8392022848129272]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bzWXSZYDpFB0"},"source":[""],"execution_count":null,"outputs":[]}]}